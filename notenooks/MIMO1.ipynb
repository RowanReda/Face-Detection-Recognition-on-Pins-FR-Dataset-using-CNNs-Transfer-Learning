{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 17423\n",
      "Bounding boxes shape: (17423, 4)\n",
      "Class labels shape: (17423, 105)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paths to the directories\n",
    "original_dir = r\"D:\\dataFaces\\archive\\105_classes_pins_dataset\"\n",
    "cropped_dir = r\"D:\\dataFaces\\cropped\"\n",
    "\n",
    "# Initialize the face detector\n",
    "detector = MTCNN()\n",
    "\n",
    "# Prepare training data\n",
    "X = []\n",
    "X = []\n",
    "y_bbox = []\n",
    "y_class_labels = []\n",
    "\n",
    "# Assign a numeric label to each celebrity folder\n",
    "celebrity_labels = {folder: idx for idx, folder in enumerate(os.listdir(original_dir))}\n",
    "num_classes = len(celebrity_labels)  # Total number of classes\n",
    "\n",
    "# Loop through each celebrity folder\n",
    "for celebrity, label in celebrity_labels.items():\n",
    "    original_celebrity_folder = os.path.join(original_dir, celebrity)\n",
    "    cropped_celebrity_folder = os.path.join(cropped_dir, celebrity)\n",
    "\n",
    "    # Process images in the original folder for bounding boxes\n",
    "    for img_file in os.listdir(original_celebrity_folder):\n",
    "        img_path = os.path.join(original_celebrity_folder, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        # Detect faces in the original image\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        faces = detector.detect_faces(img_rgb)\n",
    "\n",
    "        # If a face is detected, use the first face for simplicity\n",
    "        if faces:\n",
    "            # Bounding box for the detected face\n",
    "            x, y, w, h = faces[0]['box']\n",
    "            y_bbox.append([x, y, w, h])\n",
    "\n",
    "            # Resize the image to 224x224 for model input\n",
    "            img_resized = cv2.resize(img, (224, 224))\n",
    "            X.append(img_resized)\n",
    "            y_class_labels.append(label)\n",
    "\n",
    "# Convert lists to arrays\n",
    "X = np.array(X, dtype=\"float32\") / 255.0  # Normalize images\n",
    "y_bbox = np.array(y_bbox, dtype=\"float32\")\n",
    "y_class_labels = to_categorical(y_class_labels, num_classes=num_classes)\n",
    "\n",
    "print(\"Total images:\", X.shape[0])\n",
    "print(\"Bounding boxes shape:\", y_bbox.shape)\n",
    "print(\"Class labels shape:\", y_class_labels.shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_trainn, X_testt, y_bbox_trainn, y_bbox_testt, y_class_labels_trainn, y_class_labels_testt = train_test_split(\n",
    "    X, y_bbox, y_class_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save datasets\n",
    "np.save('X_trainn.npy', X_trainn)\n",
    "np.save('y_bbox_trainn.npy', y_bbox_trainn)\n",
    "np.save('y_class_labels_trainn.npy', y_class_labels_trainn)\n",
    "\n",
    "np.save('X_testt.npy', X_testt)\n",
    "np.save('y_bbox_testt.npy', y_bbox_testt)\n",
    "np.save('y_class_labels_testt.npy', y_class_labels_testt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trainn shape: (13938, 224, 224, 3)\n",
      "y_bbox_trainn shape: (13938, 4)\n",
      "y_class_labels_trainn shape: (13938, 105)\n",
      "X_testt shape: (3485, 224, 224, 3)\n",
      "y_bbox_testt shape: (3485, 4)\n",
      "y_class_labels_testt shape: (3485, 105)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "X_trainn = np.load('X_trainn.npy')\n",
    "y_bbox_trainn = np.load('y_bbox_trainn.npy')\n",
    "y_class_labels_trainn = np.load('y_class_labels_trainn.npy')\n",
    "\n",
    "X_testt = np.load('X_testt.npy')\n",
    "y_bbox_testt = np.load('y_bbox_testt.npy')\n",
    "y_class_labels_testt = np.load('y_class_labels_testt.npy')\n",
    "\n",
    "# Optionally, print the shapes of the loaded datasets to verify\n",
    "print(f'X_trainn shape: {X_trainn.shape}')\n",
    "print(f'y_bbox_trainn shape: {y_bbox_trainn.shape}')\n",
    "print(f'y_class_labels_trainn shape: {y_class_labels_trainn.shape}')\n",
    "\n",
    "print(f'X_testt shape: {X_testt.shape}')\n",
    "print(f'y_bbox_testt shape: {y_bbox_testt.shape}')\n",
    "print(f'y_class_labels_testt shape: {y_class_labels_testt.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (13938, 224, 224, 3)\n",
      "y_bbox_train shape: (13938, 4)\n",
      "y_class_labels_train shape: (13938, 105)\n",
      "X_test shape: (3485, 224, 224, 3)\n",
      "y_bbox_test shape: (3485, 4)\n",
      "y_class_labels_test shape: (3485, 105)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_trainn.shape)\n",
    "print(\"y_bbox_train shape:\", y_bbox_trainn.shape)\n",
    "print(\"y_class_labels_train shape:\", y_class_labels_trainn.shape)\n",
    "\n",
    "print(\"X_test shape:\", X_testt.shape)\n",
    "print(\"y_bbox_test shape:\", y_bbox_testt.shape)\n",
    "print(\"y_class_labels_test shape:\", y_class_labels_testt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Build the multi-input, multi-output model\n",
    "def build_model(input_shape=(224, 224, 3), num_classes=105):\n",
    "    # Input layer\n",
    "    input_image = Input(shape=input_shape)\n",
    "\n",
    "    # Pre-trained InceptionV3 model (without the top layers)\n",
    "    base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "    # Pass the input through the base model\n",
    "        # Pass the input through the base model\n",
    "    x = base_model(input_image)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    # x = layers.Dense(1024, activation='relu')(x)\n",
    "    # x = layers.Dropout(0.2)(x)\n",
    "    # x = layers.Dense(512, activation='elu')(x)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # Output for face detection (bounding box)\n",
    "    bbox_output = Dense(4, name=\"bbox\")(x)  # 4 values for [x, y, w, h]\n",
    "\n",
    "    # Output for celebrity recognition (class label)\n",
    "    class_output = Dense(num_classes, activation=\"softmax\", name=\"class\")(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_image, outputs=[bbox_output, class_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", \n",
    "                  loss={\"bbox\": \"mean_squared_error\", \"class\": \"categorical_crossentropy\"},\n",
    "                  metrics={\"bbox\": \"mae\", \"class\": \"accuracy\"})\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=(224, 224, 3), num_classes=105)\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Metric to monitor\n",
    "    patience=5,          # Number of epochs with no improvement to wait\n",
    "    restore_best_weights=True  # Restore the model weights from the epoch with the best value of the monitored metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4979s\u001b[0m 43s/step - bbox_loss: 5174.5107 - bbox_mae: 43.2692 - class_accuracy: 0.1051 - class_loss: 15.1064 - loss: 5189.6426\n",
      "Epoch 2/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m787s\u001b[0m 6s/step - bbox_loss: 1711.4999 - bbox_mae: 23.7227 - class_accuracy: 0.6136 - class_loss: 2.1038 - loss: 1713.6016\n",
      "Epoch 3/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 4s/step - bbox_loss: 1366.5826 - bbox_mae: 21.3363 - class_accuracy: 0.7974 - class_loss: 0.9241 - loss: 1367.5098\n",
      "Epoch 4/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 4s/step - bbox_loss: 1247.6913 - bbox_mae: 20.4635 - class_accuracy: 0.8848 - class_loss: 0.4671 - loss: 1248.1641\n",
      "Epoch 5/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 4s/step - bbox_loss: 1212.2843 - bbox_mae: 19.9752 - class_accuracy: 0.9033 - class_loss: 0.4159 - loss: 1212.7043\n",
      "Epoch 6/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 4s/step - bbox_loss: 1035.9225 - bbox_mae: 18.8132 - class_accuracy: 0.9181 - class_loss: 0.3311 - loss: 1036.2474\n",
      "Epoch 7/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 4s/step - bbox_loss: 1022.1847 - bbox_mae: 18.5917 - class_accuracy: 0.9089 - class_loss: 0.4402 - loss: 1022.6115\n",
      "Epoch 8/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 4s/step - bbox_loss: 995.5618 - bbox_mae: 18.0540 - class_accuracy: 0.8917 - class_loss: 0.5777 - loss: 996.1433\n",
      "Epoch 9/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 4s/step - bbox_loss: 923.6219 - bbox_mae: 17.5937 - class_accuracy: 0.9186 - class_loss: 0.3829 - loss: 924.0099\n",
      "Epoch 10/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 4s/step - bbox_loss: 949.1074 - bbox_mae: 17.5135 - class_accuracy: 0.9172 - class_loss: 0.4913 - loss: 949.6025\n",
      "Epoch 11/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 4s/step - bbox_loss: 888.4403 - bbox_mae: 16.9452 - class_accuracy: 0.9436 - class_loss: 0.2731 - loss: 888.7127\n",
      "Epoch 12/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 4s/step - bbox_loss: 820.2971 - bbox_mae: 16.5841 - class_accuracy: 0.9438 - class_loss: 0.2991 - loss: 820.5950\n",
      "Epoch 13/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 4s/step - bbox_loss: 792.6660 - bbox_mae: 16.2357 - class_accuracy: 0.9502 - class_loss: 0.2582 - loss: 792.9299\n",
      "Epoch 14/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 4s/step - bbox_loss: 782.4008 - bbox_mae: 16.2401 - class_accuracy: 0.9427 - class_loss: 0.3542 - loss: 782.7608\n",
      "Epoch 15/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 4s/step - bbox_loss: 787.8839 - bbox_mae: 16.0688 - class_accuracy: 0.9549 - class_loss: 0.2543 - loss: 788.1299\n",
      "Epoch 16/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 4s/step - bbox_loss: 757.0616 - bbox_mae: 15.7125 - class_accuracy: 0.9384 - class_loss: 0.3504 - loss: 757.4150\n",
      "Epoch 17/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 4s/step - bbox_loss: 721.9044 - bbox_mae: 15.4013 - class_accuracy: 0.9483 - class_loss: 0.3068 - loss: 722.2151\n",
      "Epoch 18/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 4s/step - bbox_loss: 723.2866 - bbox_mae: 15.3858 - class_accuracy: 0.9495 - class_loss: 0.3003 - loss: 723.5887\n",
      "Epoch 19/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 4s/step - bbox_loss: 672.3743 - bbox_mae: 15.0195 - class_accuracy: 0.9457 - class_loss: 0.3480 - loss: 672.7239\n",
      "Epoch 20/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 4s/step - bbox_loss: 639.1991 - bbox_mae: 14.6605 - class_accuracy: 0.9483 - class_loss: 0.3366 - loss: 639.5394\n",
      "Epoch 21/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 4s/step - bbox_loss: 687.0307 - bbox_mae: 14.8286 - class_accuracy: 0.9502 - class_loss: 0.3588 - loss: 687.3903\n",
      "Epoch 22/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 4s/step - bbox_loss: 629.5821 - bbox_mae: 14.4493 - class_accuracy: 0.9676 - class_loss: 0.2056 - loss: 629.7903\n",
      "Epoch 23/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 4s/step - bbox_loss: 636.1125 - bbox_mae: 14.4353 - class_accuracy: 0.9750 - class_loss: 0.1684 - loss: 636.2846\n",
      "Epoch 24/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 4s/step - bbox_loss: 641.7053 - bbox_mae: 14.4126 - class_accuracy: 0.9544 - class_loss: 0.3153 - loss: 642.0228\n",
      "Epoch 25/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 4s/step - bbox_loss: 589.3286 - bbox_mae: 13.9395 - class_accuracy: 0.9502 - class_loss: 0.4064 - loss: 589.7293\n",
      "Epoch 26/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 4s/step - bbox_loss: 606.9008 - bbox_mae: 14.0262 - class_accuracy: 0.9485 - class_loss: 0.4233 - loss: 607.3262\n",
      "Epoch 27/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 4s/step - bbox_loss: 591.1285 - bbox_mae: 13.8606 - class_accuracy: 0.9624 - class_loss: 0.2632 - loss: 591.3863\n",
      "Epoch 28/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 4s/step - bbox_loss: 567.1959 - bbox_mae: 13.4724 - class_accuracy: 0.9574 - class_loss: 0.3549 - loss: 567.5466\n",
      "Epoch 29/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 4s/step - bbox_loss: 562.7041 - bbox_mae: 13.5785 - class_accuracy: 0.9687 - class_loss: 0.2128 - loss: 562.9181\n",
      "Epoch 30/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 4s/step - bbox_loss: 543.2378 - bbox_mae: 13.3252 - class_accuracy: 0.9659 - class_loss: 0.2523 - loss: 543.4897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c5641f7820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(X_trainn, {\"bbox\": y_bbox_trainn, \"class\": y_class_labels_trainn}, epochs=30, batch_size=128, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 999ms/step - bbox_loss: 997.4100 - bbox_mae: 19.7283 - class_accuracy: 0.3531 - class_loss: 18.7342 - loss: 1016.1475\n",
      "Test Loss: 1001.5535888671875\n",
      "Test Bounding Box Loss: 983.0631713867188\n",
      "Test Class Loss: 18.315183639526367\n",
      "Test Bounding Box MAE: 19.534217834472656\n",
      "Test Class Accuracy: 0.37015780806541443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_bbox_loss, test_class_loss, test_bbox_mae, test_class_accuracy = model.evaluate(\n",
    "    X_testt, {\"bbox\": y_bbox_testt, \"class\": y_class_labels_testt})\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Bounding Box Loss: {test_bbox_loss}\")\n",
    "print(f\"Test Class Loss: {test_class_loss}\")\n",
    "print(f\"Test Bounding Box MAE: {test_bbox_mae}\")\n",
    "print(f\"Test Class Accuracy: {test_class_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "model.save_weights(r\"D:\\assigment5Deeplearning\\MIMO_one.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your pre-trained model\n",
    "# model = build_model(input_shape=(224, 224, 3), num_classes=105)\n",
    "# model.load_weights('MIMO_one_Weights.h5')  # Adjust the path to your trained weights\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame for the model\n",
    "    input_frame = cv2.resize(frame, (224, 224))\n",
    "    input_frame = preprocess_input(input_frame)\n",
    "    input_frame = np.expand_dims(input_frame, axis=0)\n",
    "\n",
    "    # Make predictions\n",
    "    bbox_pred, class_pred = model.predict(input_frame)\n",
    "\n",
    "    # Extract bounding box and class predictions\n",
    "    x, y, w, h = bbox_pred[0]  # bounding box coordinates\n",
    "    class_id = np.argmax(class_pred[0])  # Get class ID with highest probability\n",
    "    class_confidence = class_pred[0][class_id]  # Probability of the predicted class\n",
    "\n",
    "    # Convert bounding box to original frame scale\n",
    "    height, width, _ = frame.shape\n",
    "    x, y, w, h = int(x * width), int(y * height), int(w * width), int(h * height)\n",
    "\n",
    "    # Draw bounding box and label on the frame\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    label = f\"Class: {class_id}, Confidence: {class_confidence:.2f}\"\n",
    "    cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Live Face Detection and Classification\", frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
